name: Deploy Staging Environment

on:
  push:
    branches:
      - staging

# CONCURRENCY: Cancel older, redundant runs to save runner costs
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

env:
  AWS_REGION: us-east-1
  ECR_REGISTRY: 569758639273.dkr.ecr.us-east-1.amazonaws.com
  STAGING_API_URL: https://api.staging.rwako.com

permissions:
  contents: read
  id-token: write

jobs:
  # =========================================================
  # JOB 1: BUILD & DEPLOY TO STAGING EKS
  # =========================================================
  deploy-staging:
    name: Build & Deploy to Staging
    runs-on: [self-hosted, linux, x64, saas-ci]

    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          # Note: In a real setup, you should create a dedicated saas-github-actions-staging-role
          # so staging cannot touch production resources!
          role-to-assume: arn:aws:iam::569758639273:role/saas-github-actions-deploy-role
          aws-region: us-east-1

      - name: Login to Amazon ECR
        uses: aws-actions/amazon-ecr-login@v2

      # 1. BUILD BACKEND & FRONTEND WITH COMMIT SHA TAG
      - name: Build and Push Backend Image
        working-directory: ./backend
        run: |
          IMAGE_TAG=staging-${{ github.sha }}
          docker build --platform linux/amd64 -t ${{ env.ECR_REGISTRY }}/saas-backend:$IMAGE_TAG .
          docker push ${{ env.ECR_REGISTRY }}/saas-backend:$IMAGE_TAG

      - name: Build and Push Frontend Image
        working-directory: ./frontend
        run: |
          IMAGE_TAG=staging-${{ github.sha }}
          docker build \
            --platform linux/amd64 \
            --build-arg VITE_API_URL=${{ env.STAGING_API_URL }} \
            -t ${{ env.ECR_REGISTRY }}/saas-frontend:$IMAGE_TAG .
          docker push ${{ env.ECR_REGISTRY }}/saas-frontend:$IMAGE_TAG

      # 2. RUN DATABASE MIGRATIONS ON STAGING DB
      - name: Setup Node
        uses: actions/setup-node@v4
        with:
          node-version: '22'
          cache: 'npm'
          cache-dependency-path: './backend/package-lock.json'

      - name: Install AWS CLI
        run: |
          if ! command -v aws &> /dev/null; then
            echo "AWS CLI not found, installing..."
            sudo apt-get update && sudo apt-get install -y unzip
            curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
            unzip -q awscliv2.zip
            sudo ./aws/install --update
            rm -rf awscliv2.zip aws/
          else
            echo "AWS CLI is already installed"
          fi

      - name: Run Database Migrations
        working-directory: ./backend
        run: |
          # Fetch the staging database URL from SSM Parameter Store dynamically
          DATABASE_URL=$(aws ssm get-parameter --name "/saas/staging/database/url" --with-decryption --query "Parameter.Value" --output text)

          if [ -z "$DATABASE_URL" ]; then
            echo "Error: Failed to fetch DATABASE_URL from SSM"
            exit 1
          fi

          export DATABASE_URL=$DATABASE_URL

          # Debug: Show the hostname being used (masking password)
          echo "Connecting to: $(echo $DATABASE_URL | sed 's/:.*@/:****@/')"

          npm ci
          npm run migrate

      - name: Install Kustomize
        run: |
          if ! command -v kustomize &> /dev/null; then
            echo "Kustomize not found, installing..."
            curl -s "https://raw.githubusercontent.com/kubernetes-sigs/kustomize/master/hack/install_kustomize.sh" | bash
            sudo mv kustomize /usr/local/bin/
          else
            echo "Kustomize is already installed"
          fi

      # 3. DEPLOY TO KUBERNETES USING KUSTOMIZE
      - name: Deploy to Staging Cluster
        working-directory: ./kubernetes/overlays/staging
        run: |
          # Connect to the staging cluster (Ensure cluster name matches eks.tf)
          aws eks update-kubeconfig --name saas-staging-cluster --region us-east-1

          # Update the kustomization.yaml to point to the newly built image tags!
          kustomize edit set image 569758639273.dkr.ecr.us-east-1.amazonaws.com/saas-backend=569758639273.dkr.ecr.us-east-1.amazonaws.com/saas-backend:staging-${{ github.sha }}
          kustomize edit set image 569758639273.dkr.ecr.us-east-1.amazonaws.com/saas-frontend=569758639273.dkr.ecr.us-east-1.amazonaws.com/saas-frontend:staging-${{ github.sha }}

          # Apply the manifests into the staging namespace
          kubectl kustomize . | kubectl apply -n staging -f -

          # Wait for the rollout to complete
          kubectl rollout status deployment/saas-backend -n staging --timeout=120s
          kubectl rollout status deployment/saas-frontend -n staging --timeout=120s
