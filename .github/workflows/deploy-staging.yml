name: Deploy Staging Environment

on:
  push:
    branches:
      - staging
  pull_request:
    branches:
      - staging

# CONCURRENCY: Cancel older, redundant runs to save runner costs
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

env:
  AWS_REGION: us-east-1
  ECR_REGISTRY: 569758639273.dkr.ecr.us-east-1.amazonaws.com
  STAGING_API_URL: https://api.staging.rwako.com

permissions:
  contents: read
  id-token: write

jobs:
  # =========================================================
  # JOB 1: BUILD & DEPLOY TO STAGING EKS
  # =========================================================
  deploy-staging:
    name: Build & Deploy to Staging
    runs-on: [self-hosted, linux, x64, saas-ci]

    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          # Note: In a real setup, you should create a dedicated saas-github-actions-staging-role
          # so staging cannot touch production resources!
          role-to-assume: arn:aws:iam::569758639273:role/saas-staging-github-deploy-role
          aws-region: us-east-1

      - name: Login to Amazon ECR
        uses: aws-actions/amazon-ecr-login@v2

      - name: Install AWS CLI
        run: |
          if ! command -v aws &> /dev/null; then
            echo "AWS CLI not found, installing..."
            sudo apt-get update && sudo apt-get install -y unzip
            curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
            unzip -q awscliv2.zip
            sudo ./aws/install --update
            rm -rf awscliv2.zip aws/
          else
            echo "AWS CLI is already installed"
          fi

      - name: Install Kubectl and Kustomize
        run: |
          if ! command -v kubectl &> /dev/null; then
            echo "Kubectl not found, installing..."
            curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
            sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
            rm kubectl
          fi

          if ! command -v kustomize &> /dev/null; then
            echo "Kustomize not found, installing..."
            curl -s "https://raw.githubusercontent.com/kubernetes-sigs/kustomize/master/hack/install_kustomize.sh" | bash
            sudo mv kustomize /usr/local/bin/
          fi

      # 1. BUILD BACKEND & FRONTEND WITH COMMIT SHA TAG
      - name: Build and Push Backend Image
        working-directory: ./backend
        run: |
          IMAGE_TAG=staging-${{ github.sha }}
          docker build --platform linux/amd64 -t ${{ env.ECR_REGISTRY }}/saas-staging-backend:$IMAGE_TAG .
          docker push ${{ env.ECR_REGISTRY }}/saas-staging-backend:$IMAGE_TAG

      - name: Build and Push Frontend Image
        working-directory: ./frontend
        run: |
          IMAGE_TAG=staging-${{ github.sha }}
          docker build \
            --platform linux/amd64 \
            --build-arg VITE_API_URL=${{ env.STAGING_API_URL }} \
            -t ${{ env.ECR_REGISTRY }}/saas-staging-frontend:$IMAGE_TAG .
          docker push ${{ env.ECR_REGISTRY }}/saas-staging-frontend:$IMAGE_TAG

      # 2. RUN DATABASE MIGRATIONS ON STAGING DB
      - name: Run Database Migrations
        working-directory: ./kubernetes/overlays/staging
        run: |
          aws eks update-kubeconfig --name saas-staging-cluster --region us-east-1

          # MANUAL CRD INSTALLATION:
          # The Helm chart seems to have failed to install the ExternalSecret CRD specifically.
          # We force install the full official CRD bundle to ensure all types are available.
          # We use --server-side to bypass the 256KB annotation limit for large CRDs.
          echo "Installing External Secrets CRD bundle..."
          kubectl apply -f https://raw.githubusercontent.com/external-secrets/external-secrets/main/deploy/crds/bundle.yaml --server-side --force-conflicts

          # Update the kustomization.yaml to point to the newly built image tags!
          kustomize edit set image 569758639273.dkr.ecr.us-east-1.amazonaws.com/saas-backend=569758639273.dkr.ecr.us-east-1.amazonaws.com/saas-staging-backend:staging-${{ github.sha }}
          kustomize edit set image 569758639273.dkr.ecr.us-east-1.amazonaws.com/saas-frontend=569758639273.dkr.ecr.us-east-1.amazonaws.com/saas-staging-frontend:staging-${{ github.sha }}

          # Delete old job if it exists
          kubectl delete job saas-db-migrate -n staging --ignore-not-found

          # FIX: Force IRSA Annotation and Operator Restart
          # The logs show the ServiceAccount is missing the role association.
          echo "Applying IRSA fix to external-secrets ServiceAccount..."
          ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          kubectl annotate serviceaccount external-secrets -n external-secrets \
            eks.amazonaws.com/role-arn=arn:aws:iam::${ACCOUNT_ID}:role/saas-staging-eks-ssm-secrets-reader --overwrite

          echo "Restarting External Secrets Operator to pick up IRSA credentials..."
          kubectl rollout restart deployment external-secrets -n external-secrets
          kubectl rollout status deployment external-secrets -n external-secrets --timeout=60s

          # Apply manifests with --validate=false to bypass potential discovery cache lag
          kubectl apply -k . --validate=false

          # WAIT for ExternalSecrets to sync from AWS with diagnostic fallback
          echo "Waiting for secrets to sync from AWS..."
          if ! kubectl wait --for=condition=Ready externalsecret/saas-auth-sync -n staging --timeout=120s; then
            echo "❌ ERROR: Timeout waiting for saas-auth-sync"
            echo "--- ExternalSecret Status ---"
            kubectl get externalsecret saas-auth-sync -n staging -o yaml
            echo "--- ExternalSecret Events ---"
            kubectl get events -n staging --field-selector involvedObject.name=saas-auth-sync
            echo "--- Operator Logs ---"
            kubectl logs -l app.kubernetes.io/name=external-secrets -n external-secrets --tail=50
            exit 1
          fi

          if ! kubectl wait --for=condition=Ready externalsecret/saas-db-sync -n staging --timeout=120s; then
            echo "❌ ERROR: Timeout waiting for saas-db-sync"
            echo "--- ExternalSecret Status ---"
            kubectl get externalsecret saas-db-sync -n staging -o yaml
            echo "--- ExternalSecret Events ---"
            kubectl get events -n staging --field-selector involvedObject.name=saas-db-sync
            exit 1
          fi

          # PRE-FLIGHT: Check that we can reach the RDS endpoint before wasting job retries.
          # This runner is self-hosted inside the EKS cluster, so this test validates
          # the SAME network path that the migration pod will use.
          echo "--- Pre-flight: Checking RDS connectivity from runner ---"
          DB_URL=$(kubectl get secret saas-db-secret -n staging -o jsonpath='{.data.database_url}' | base64 -d)
          DB_HOST=$(echo "$DB_URL" | sed -E 's|postgres://[^@]+@([^:/]+).*|\1|')
          DB_PORT=$(echo "$DB_URL" | sed -E 's|postgres://[^@]+@[^:]+:([0-9]+).*|\1|')
          DB_PORT=${DB_PORT:-5432}
          echo "RDS target → Host: ${DB_HOST}, Port: ${DB_PORT}"
          # Use bash's built-in /dev/tcp — works everywhere, no nc needed.
          if timeout 10 bash -c "echo >/dev/tcp/${DB_HOST}/${DB_PORT}" 2>&1; then
            echo "✅ TCP connection to ${DB_HOST}:${DB_PORT} succeeded"
          else
            echo "❌ FATAL: Cannot reach RDS at ${DB_HOST}:${DB_PORT}"
            echo "This means the security group or routing between EKS and RDS is broken."
            echo "Check: aws ec2 describe-security-groups for the RDS instance."
            exit 1
          fi


          # Wait for the migration job to finish with diagnostic fallback.
          # We check for EITHER complete (success) OR failed (BackoffLimitExceeded).
          echo "Waiting for database migration job to complete..."
          if ! kubectl wait --for=condition=complete job/saas-db-migrate -n staging --timeout=300s; then
            echo "❌ ERROR: Migration job did not complete successfully"
            echo "--- Job Status ---"
            kubectl get job saas-db-migrate -n staging -o yaml
            echo "--- Job Pod Listing (including terminated) ---"
            kubectl get pods -l job-name=saas-db-migrate -n staging -o wide --show-all 2>&1 || \
              kubectl get pods -l job-name=saas-db-migrate -n staging -o wide 2>&1 || true

            # Read termination messages from pod status JSON.
            # Kubernetes stores the last 4KB of /dev/termination-log in pod status,
            # so these messages persist even after the container has exited/been GC'd.
            echo "--- Termination Messages (from pod status, survives pod GC) ---"
            kubectl get pods -l job-name=saas-db-migrate -n staging -o json 2>/dev/null \
              | jq -r '.items[] | .metadata.name as $pod | .status.containerStatuses[]? | ($pod + " | exit=" + ((.lastState.terminated.exitCode // .state.terminated.exitCode // "?") | tostring) + " | reason=" + (.lastState.terminated.reason // .state.terminated.reason // "?") + "\nOutput:\n" + (.lastState.terminated.message // .state.terminated.message // "(no message)"))' \
              2>&1 || echo "(jq not available or no terminated pods found)"


            # If pods still exist, also try direct log access
            for POD in $(kubectl get pods -l job-name=saas-db-migrate -n staging -o jsonpath='{.items[*].metadata.name}' 2>/dev/null); do
              echo "--- Logs for pod: ${POD} ---"
              kubectl logs "${POD}" -n staging -c migrate --tail=200 2>&1 || true
              echo "--- Previous container logs for pod: ${POD} ---"
              kubectl logs "${POD}" -n staging -c migrate --previous --tail=200 2>&1 || true
            done
            exit 1
          fi

      # 3. DEPLOY TO KUBERNETES USING KUSTOMIZE
      - name: Deploy to Staging Cluster
        working-directory: ./kubernetes/overlays/staging
        run: |
          # The apply happened in the migration step, but we run it again
          # here just to be sure and to perform the final verification.
          kubectl apply -k . --validate=false -n staging

          # Wait for the rollout to complete
          kubectl rollout status deployment/saas-backend -n staging --timeout=120s
          kubectl rollout status deployment/saas-frontend -n staging --timeout=120s
