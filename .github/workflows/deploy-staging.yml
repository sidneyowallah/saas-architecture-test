name: Deploy Staging Environment

on:
  push:
    branches:
      - staging
  pull_request:
    branches:
      - staging

# CONCURRENCY: Cancel older, redundant runs to save runner costs
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

env:
  AWS_REGION: us-east-1
  ECR_REGISTRY: 569758639273.dkr.ecr.us-east-1.amazonaws.com
  STAGING_API_URL: https://api.staging.rwako.com

permissions:
  contents: read
  id-token: write

jobs:
  # =========================================================
  # JOB 1: BUILD & DEPLOY TO STAGING EKS
  # =========================================================
  deploy-staging:
    name: Build & Deploy to Staging
    runs-on: [self-hosted, linux, x64, saas-ci]

    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          # Note: In a real setup, you should create a dedicated saas-github-actions-staging-role
          # so staging cannot touch production resources!
          role-to-assume: arn:aws:iam::569758639273:role/saas-staging-github-deploy-role
          aws-region: us-east-1

      - name: Login to Amazon ECR
        uses: aws-actions/amazon-ecr-login@v2

      - name: Install AWS CLI
        run: |
          if ! command -v aws &> /dev/null; then
            echo "AWS CLI not found, installing..."
            sudo apt-get update && sudo apt-get install -y unzip
            curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
            unzip -q awscliv2.zip
            sudo ./aws/install --update
            rm -rf awscliv2.zip aws/
          else
            echo "AWS CLI is already installed"
          fi

      - name: Install Kubectl and Kustomize
        run: |
          if ! command -v kubectl &> /dev/null; then
            echo "Kubectl not found, installing..."
            curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
            sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
            rm kubectl
          fi

          if ! command -v kustomize &> /dev/null; then
            echo "Kustomize not found, installing..."
            curl -s "https://raw.githubusercontent.com/kubernetes-sigs/kustomize/master/hack/install_kustomize.sh" | bash
            sudo mv kustomize /usr/local/bin/
          fi

      # 1. BUILD BACKEND & FRONTEND WITH COMMIT SHA TAG
      - name: Build and Push Backend Image
        working-directory: ./backend
        run: |
          IMAGE_TAG=staging-${{ github.sha }}
          docker build --platform linux/amd64 -t ${{ env.ECR_REGISTRY }}/saas-staging-backend:$IMAGE_TAG .
          docker push ${{ env.ECR_REGISTRY }}/saas-staging-backend:$IMAGE_TAG

      - name: Build and Push Frontend Image
        working-directory: ./frontend
        run: |
          IMAGE_TAG=staging-${{ github.sha }}
          docker build \
            --platform linux/amd64 \
            --build-arg VITE_API_URL=${{ env.STAGING_API_URL }} \
            -t ${{ env.ECR_REGISTRY }}/saas-staging-frontend:$IMAGE_TAG .
          docker push ${{ env.ECR_REGISTRY }}/saas-staging-frontend:$IMAGE_TAG

      # 2. RUN DATABASE MIGRATIONS ON STAGING DB
      - name: Run Database Migrations
        working-directory: ./kubernetes/overlays/staging
        run: |
          aws eks update-kubeconfig --name saas-staging-cluster --region us-east-1

          # MANUAL CRD INSTALLATION:
          # The Helm chart seems to have failed to install the ExternalSecret CRD specifically.
          # We force install the full official CRD bundle to ensure all types are available.
          # We use --server-side to bypass the 256KB annotation limit for large CRDs.
          echo "Installing External Secrets CRD bundle..."
          kubectl apply -f https://raw.githubusercontent.com/external-secrets/external-secrets/main/deploy/crds/bundle.yaml --server-side --force-conflicts

          # Update the kustomization.yaml to point to the newly built image tags!
          kustomize edit set image 569758639273.dkr.ecr.us-east-1.amazonaws.com/saas-backend=569758639273.dkr.ecr.us-east-1.amazonaws.com/saas-staging-backend:staging-${{ github.sha }}
          kustomize edit set image 569758639273.dkr.ecr.us-east-1.amazonaws.com/saas-frontend=569758639273.dkr.ecr.us-east-1.amazonaws.com/saas-staging-frontend:staging-${{ github.sha }}

          # Delete old job if it exists
          kubectl delete job saas-db-migrate -n staging --ignore-not-found

          # Apply manifests with --validate=false to bypass potential discovery cache lag
          kubectl apply -k . --validate=false
          kubectl wait --for=condition=complete job/saas-db-migrate -n staging --timeout=300s

      # 3. DEPLOY TO KUBERNETES USING KUSTOMIZE
      - name: Deploy to Staging Cluster
        working-directory: ./kubernetes/overlays/staging
        run: |
          # The apply happened in the migration step, but we run it again
          # here just to be sure and to perform the final verification.
          kubectl apply -k . --validate=false -n staging

          # Wait for the rollout to complete
          kubectl rollout status deployment/saas-backend -n staging --timeout=120s
          kubectl rollout status deployment/saas-frontend -n staging --timeout=120s
