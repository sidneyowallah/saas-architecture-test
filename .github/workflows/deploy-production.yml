name: Deploy Production Environment

on:
  push:
    branches:
      - main
  pull_request:
    branches:
      - main

# 1. CONCURRENCY: Cancel older, redundant runs to save EC2 Spot instance costs
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

# 2. PERMISSIONS: Enforce least privilege for the GitHub token
permissions:
  contents: read

env:
  AWS_REGION: us-east-1
  ECR_REGISTRY: 569758639273.dkr.ecr.us-east-1.amazonaws.com
  PROD_API_URL: https://api.rwako.com

jobs:
  # =========================================================
  # JOB 1: BACKEND CI (Lint, Test, & TypeScript Build)
  # =========================================================
  backend-ci:
    name: Backend CI
    runs-on: [self-hosted, linux, x64, saas-ci]
    defaults:
      run:
        working-directory: ./backend

    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      # 3. NODE 22: Upgraded to modern LTS standard
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: "22"
          cache: "npm"
          cache-dependency-path: "./backend/package-lock.json"

      - name: Install Dependencies
        run: npm ci

      - name: Enforce Code Formatting (Prettier)
        run: npx prettier --check "**/*.ts"

      - name: TypeScript Type Check
        run: npx tsc --noEmit

      - name: Run Unit Tests
        run: npm run test --if-present

  # =========================================================
  # JOB 2: FRONTEND CI (Lint, Test, & Vite Build)
  # =========================================================
  frontend-ci:
    name: Frontend CI
    runs-on: [self-hosted, linux, x64, saas-ci]
    defaults:
      run:
        working-directory: ./frontend

    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: "22"
          cache: "npm"
          cache-dependency-path: "./frontend/package-lock.json"

      - name: Install Dependencies
        run: npm ci

      - name: Enforce Code Formatting (Prettier)
        run: npx prettier --check "**/*.{ts,tsx}"

      - name: Build Vite App
        run: npm run build

  # =========================================================
  # JOB 3: DEPLOY TO AWS (Migrate DB, Build Images, Update ECS)
  # =========================================================
  deploy-to-aws:
    name: Deploy to Production
    needs: [backend-ci, frontend-ci]
    # Only run deployment if the code is merged/pushed to main
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    runs-on: [self-hosted, linux, x64, saas-ci]

    permissions:
      id-token: write
      contents: read

    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: "22"
          cache: "npm"
          cache-dependency-path: "./backend/package-lock.json"

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::569758639273:role/saas-production-github-deploy-role
          aws-region: us-east-1

      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Install AWS CLI
        run: |
          if ! command -v aws &> /dev/null; then
            echo "AWS CLI not found, installing..."
            sudo apt-get update && sudo apt-get install -y unzip
            curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
            unzip -q awscliv2.zip
            sudo ./aws/install --update
            rm -rf awscliv2.zip aws/
          else
            echo "AWS CLI is already installed"
          fi

      - name: Install Kubectl and Kustomize
        run: |
          if ! command -v kubectl &> /dev/null; then
            echo "Kubectl not found, installing..."
            curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
            sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
            rm kubectl
          fi

          if ! command -v kustomize &> /dev/null; then
            echo "Kustomize not found, installing..."
            curl -s "https://raw.githubusercontent.com/kubernetes-sigs/kustomize/master/hack/install_kustomize.sh" | bash
            sudo mv kustomize /usr/local/bin/
          fi

      - name: Build and Push Backend Image
        working-directory: ./backend
        run: |
          IMAGE_TAG=${{ github.sha }}
          REPOSITORY=saas-production-backend

          docker build --platform linux/amd64 -t $ECR_REGISTRY/$REPOSITORY:$IMAGE_TAG .
          docker push $ECR_REGISTRY/$REPOSITORY:$IMAGE_TAG

          # Tag as latest for easy reference
          docker tag $ECR_REGISTRY/$REPOSITORY:$IMAGE_TAG $ECR_REGISTRY/$REPOSITORY:latest
          docker push $ECR_REGISTRY/$REPOSITORY:latest

      # 5. FRONTEND BUILD ENV: Inject the production API URL during Docker build
      - name: Build and Push Frontend Image
        working-directory: ./frontend
        run: |
          IMAGE_TAG=${{ github.sha }}
          REPOSITORY=saas-production-frontend

          docker build \
            --platform linux/amd64 \
            --build-arg VITE_API_URL=${{ env.PROD_API_URL }} \
            -t $ECR_REGISTRY/$REPOSITORY:$IMAGE_TAG .
            
          docker push $ECR_REGISTRY/$REPOSITORY:$IMAGE_TAG

      # 6. DATABASE MIGRATIONS: Run safely inside the VPC using a Kubernetes Job
      - name: Run Database Migrations
        working-directory: ./kubernetes/overlays/production
        run: |
          aws eks update-kubeconfig --name saas-production-cluster --region us-east-1

          # FORCE REFRESH of the kubectl discovery cache!
          # This prevents "no matches for kind" errors for newly installed CRDs.
          kubectl api-resources > /dev/null

          # Update the kustomization.yaml to point to the newly built image tags!
          # This ensures the migration job uses the latest backend image.
          kustomize edit set image 569758639273.dkr.ecr.us-east-1.amazonaws.com/saas-backend=569758639273.dkr.ecr.us-east-1.amazonaws.com/saas-production-backend:${{ github.sha }}
          kustomize edit set image 569758639273.dkr.ecr.us-east-1.amazonaws.com/saas-frontend=569758639273.dkr.ecr.us-east-1.amazonaws.com/saas-production-frontend:${{ github.sha }}

          # Delete old job if it exists to allow re-run
          kubectl delete job saas-db-migrate -n production --ignore-not-found

          # Apply manifests and wait for the migration job to finish
          kubectl apply -k .
          kubectl wait --for=condition=complete job/saas-db-migrate -n production --timeout=300s

      - name: Deploy to Production
        working-directory: ./kubernetes/overlays/production
        run: |
          # The apply happened in the migration step, but we run it again
          # here just to be sure and to perform the final verification.
          kubectl apply -k . --validate=false

          # 4. Wait for the rollout to complete in the production namespace
          kubectl rollout status deployment/saas-backend -n production --timeout=120s
          kubectl rollout status deployment/saas-frontend -n production --timeout=120s
