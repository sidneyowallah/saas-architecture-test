name: Deploy Production Environment

on:
  push:
    branches:
      - main
  pull_request:
    branches:
      - main

# 1. CONCURRENCY: Cancel older, redundant runs to save EC2 Spot instance costs
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

# 2. PERMISSIONS: Enforce least privilege for the GitHub token
permissions:
  contents: read

env:
  AWS_REGION: us-east-1
  ECR_REGISTRY: 569758639273.dkr.ecr.us-east-1.amazonaws.com
  PROD_API_URL: https://api.rwako.com

jobs:
  # =========================================================
  # JOB 1: BACKEND CI (Lint, Test, & TypeScript Build)
  # =========================================================
  backend-ci:
    name: Backend CI
    runs-on: [self-hosted, linux, x64, saas-ci]
    defaults:
      run:
        working-directory: ./backend

    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      # 3. NODE 22: Upgraded to modern LTS standard
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: "22"
          cache: "npm"
          cache-dependency-path: "./backend/package-lock.json"

      - name: Install Dependencies
        run: npm ci

      - name: Enforce Code Formatting (Prettier)
        run: npx prettier --check "**/*.ts"

      - name: TypeScript Type Check
        run: npx tsc --noEmit

      - name: Run Unit Tests
        run: npm run test --if-present

  # =========================================================
  # JOB 2: FRONTEND CI (Lint, Test, & Vite Build)
  # =========================================================
  frontend-ci:
    name: Frontend CI
    runs-on: [self-hosted, linux, x64, saas-ci]
    defaults:
      run:
        working-directory: ./frontend

    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: "22"
          cache: "npm"
          cache-dependency-path: "./frontend/package-lock.json"

      - name: Install Dependencies
        run: npm ci

      - name: Enforce Code Formatting (Prettier)
        run: npx prettier --check "**/*.{ts,tsx}"

      - name: Build Vite App
        run: npm run build

  # =========================================================
  # JOB 3: DEPLOY TO AWS (Migrate DB, Build Images, Update ECS)
  # =========================================================
  deploy-to-aws:
    name: Deploy to Production
    needs: [backend-ci, frontend-ci]
    # Only run deployment if the code is merged/pushed to main
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    runs-on: [self-hosted, linux, x64, saas-ci]

    permissions:
      id-token: write
      contents: read

    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: "22"
          cache: "npm"
          cache-dependency-path: "./backend/package-lock.json"

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::569758639273:role/saas-production-github-deploy-role
          aws-region: us-east-1

      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Install AWS CLI
        run: |
          if ! command -v aws &> /dev/null; then
            echo "AWS CLI not found, installing..."
            sudo apt-get update && sudo apt-get install -y unzip
            curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
            unzip -q awscliv2.zip
            sudo ./aws/install --update
            rm -rf awscliv2.zip aws/
          else
            echo "AWS CLI is already installed"
          fi

      - name: Install Kubectl and Kustomize
        run: |
          if ! command -v kubectl &> /dev/null; then
            echo "Kubectl not found, installing..."
            curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
            sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
            rm kubectl
          fi

          if ! command -v kustomize &> /dev/null; then
            echo "Kustomize not found, installing..."
            curl -s "https://raw.githubusercontent.com/kubernetes-sigs/kustomize/master/hack/install_kustomize.sh" | bash
            sudo mv kustomize /usr/local/bin/
          fi

      - name: Build and Push Backend Image
        working-directory: ./backend
        run: |
          IMAGE_TAG=${{ github.sha }}
          REPOSITORY=saas-production-backend

          docker build --platform linux/amd64 -t $ECR_REGISTRY/$REPOSITORY:$IMAGE_TAG .
          docker push $ECR_REGISTRY/$REPOSITORY:$IMAGE_TAG

          # Tag as latest for easy reference
          docker tag $ECR_REGISTRY/$REPOSITORY:$IMAGE_TAG $ECR_REGISTRY/$REPOSITORY:latest
          docker push $ECR_REGISTRY/$REPOSITORY:latest

      # 5. FRONTEND BUILD ENV: Inject the production API URL during Docker build
      - name: Build and Push Frontend Image
        working-directory: ./frontend
        run: |
          IMAGE_TAG=${{ github.sha }}
          REPOSITORY=saas-production-frontend

          docker build \
            --platform linux/amd64 \
            --build-arg VITE_API_URL=${{ env.PROD_API_URL }} \
            -t $ECR_REGISTRY/$REPOSITORY:$IMAGE_TAG .
            
          docker push $ECR_REGISTRY/$REPOSITORY:$IMAGE_TAG

      # 6. DATABASE MIGRATIONS: Run safely inside the VPC using a Kubernetes Job
      - name: Run Database Migrations
        working-directory: ./kubernetes/overlays/production
        run: |
          aws eks update-kubeconfig --name saas-production-cluster --region us-east-1

          # MANUAL CRD INSTALLATION:
          # The Helm chart seems to have failed to install the ExternalSecret CRD specifically.
          # We force install the full official CRD bundle to ensure all types are available.
          # We use --server-side to bypass the 256KB annotation limit for large CRDs.
          echo "Installing External Secrets CRD bundle..."
          kubectl apply -f https://raw.githubusercontent.com/external-secrets/external-secrets/main/deploy/crds/bundle.yaml --server-side --force-conflicts

          # Update the kustomization.yaml to point to the newly built image tags!
          # This ensures the migration job uses the latest backend image.
          kustomize edit set image 569758639273.dkr.ecr.us-east-1.amazonaws.com/saas-backend=569758639273.dkr.ecr.us-east-1.amazonaws.com/saas-production-backend:${{ github.sha }}
          kustomize edit set image 569758639273.dkr.ecr.us-east-1.amazonaws.com/saas-frontend=569758639273.dkr.ecr.us-east-1.amazonaws.com/saas-production-frontend:${{ github.sha }}

          # Delete old job if it exists to allow re-run
          kubectl delete job saas-db-migrate -n production --ignore-not-found

          # FIX: Force IRSA Annotation and Operator Restart
          # The logs show the ServiceAccount is missing the role association.
          echo "Applying IRSA fix to external-secrets ServiceAccount..."
          ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          kubectl annotate serviceaccount external-secrets -n external-secrets \
            eks.amazonaws.com/role-arn=arn:aws:iam::${ACCOUNT_ID}:role/saas-production-eks-ssm-secrets-reader --overwrite

          echo "Restarting External Secrets Operator to pick up IRSA credentials..."
          kubectl rollout restart deployment external-secrets -n external-secrets
          kubectl rollout status deployment external-secrets -n external-secrets --timeout=60s

          # Apply manifests with --validate=false to bypass potential discovery cache lag
          kubectl apply -k . --validate=false

          # WAIT for ExternalSecrets to sync from AWS with diagnostic fallback
          echo "Waiting for secrets to sync from AWS..."
          if ! kubectl wait --for=condition=Ready externalsecret/saas-auth-sync -n production --timeout=120s; then
            echo "❌ ERROR: Timeout waiting for saas-auth-sync"
            echo "--- ExternalSecret Status ---"
            kubectl get externalsecret saas-auth-sync -n production -o yaml
            echo "--- ExternalSecret Events ---"
            kubectl get events -n production --field-selector involvedObject.name=saas-auth-sync
            echo "--- Operator Logs ---"
            kubectl logs -l app.kubernetes.io/name=external-secrets -n external-secrets --tail=50
            exit 1
          fi

          if ! kubectl wait --for=condition=Ready externalsecret/saas-db-sync -n production --timeout=120s; then
            echo "❌ ERROR: Timeout waiting for saas-db-sync"
            echo "--- ExternalSecret Status ---"
            kubectl get externalsecret saas-db-sync -n production -o yaml
            echo "--- ExternalSecret Events ---"
            kubectl get events -n production --field-selector involvedObject.name=saas-db-sync
            exit 1
          fi

          # Wait for the migration job to finish with diagnostic fallback
          echo "Waiting for database migration job to complete..."
          if ! kubectl wait --for=condition=complete job/saas-db-migrate -n production --timeout=300s; then
            echo "❌ ERROR: Timeout waiting for saas-db-migrate job"
            echo "--- Job Status ---"
            kubectl get job saas-db-migrate -n production -o yaml
            echo "--- Job Logs ---"
            kubectl logs job/saas-db-migrate -n production --tail=100
            echo "--- Job Pod Events ---"
            JOB_POD=$(kubectl get pods -l job-name=saas-db-migrate -n production -o jsonpath='{.items[0].metadata.name}' --ignore-not-found)
            if [ -n "$JOB_POD" ]; then
              kubectl get events -n production --field-selector involvedObject.name=$JOB_POD
            fi
            exit 1
          fi

      - name: Deploy to Production
        working-directory: ./kubernetes/overlays/production
        run: |
          # The apply happened in the migration step, but we run it again
          # here just to be sure and to perform the final verification.
          kubectl apply -k . --validate=false

          # 4. Wait for the rollout to complete in the production namespace
          kubectl rollout status deployment/saas-backend -n production --timeout=120s
          kubectl rollout status deployment/saas-frontend -n production --timeout=120s
